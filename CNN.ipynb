{"cells":[{"cell_type":"markdown","id":"ed554c61-a57d-41a0-abe0-1fb47b01565b","metadata":{"id":"ed554c61-a57d-41a0-abe0-1fb47b01565b"},"source":["# **Implimentation of CNN, for classifying the images of Dogs and Cats**"]},{"cell_type":"markdown","id":"a9f4c75f-165f-41ac-bf88-f50328c75210","metadata":{"id":"a9f4c75f-165f-41ac-bf88-f50328c75210"},"source":["# (1) Importing the Libraries"]},{"cell_type":"code","execution_count":null,"id":"5badfe71-e5f2-4046-a7aa-5533c335fd49","metadata":{"id":"5badfe71-e5f2-4046-a7aa-5533c335fd49"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","# From keras library, we are importing this class\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"id":"ed40119f-7dbb-4d42-9bc4-b5b8d2bb2d2b","metadata":{"id":"ed40119f-7dbb-4d42-9bc4-b5b8d2bb2d2b","outputId":"121d0807-a720-4ef3-960b-d16fb6471725"},"outputs":[{"data":{"text/plain":["'2.13.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","id":"5ed6e787-1302-453e-aa63-6186af19c3d1","metadata":{"id":"5ed6e787-1302-453e-aa63-6186af19c3d1"},"source":["# (2) Data Preprocessing"]},{"cell_type":"markdown","id":"6e067623-572e-4b07-b964-b77bccd9cfdd","metadata":{"id":"6e067623-572e-4b07-b964-b77bccd9cfdd"},"source":["## Preprocessing the Training Set"]},{"cell_type":"code","execution_count":null,"id":"dc397ab7-44c8-432d-9528-4ccedc6428bd","metadata":{"id":"dc397ab7-44c8-432d-9528-4ccedc6428bd","outputId":"be71b30f-37f7-4985-bfe0-edf42c6da804"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8000 images belonging to 2 classes.\n"]}],"source":["# Firstly we gonna apply some tranformations on the all the images on the training Set only wonot apply the same Transformation on Test set bcz to avoid the overfitting\n","# This would do random transformation on the training Images, so that it would make our TrainingImages more diverse, which avoid the overfitting of our model\n","# This randomness introduces variability into the training data. As a result, each time you generate a batch of augmented data, you get a slightly different set of images, which helps the model learn to handle variations and generalize better.\n","# The class included above is very convienece as bcz it would does those random transformation on the images in realTime as the images fed to the model\n","# Its just like scaling ass it's only done on training set for better feature extraction and learning, not on testing set.\n","\n","# Build the instance of ImageDataGeneretor-Class / the class's object\n","train_datagen= ImageDataGenerator(\n","         rescale=1./255, # Its feature Scaling, in which every pixel by dividing their value by 255, so that each would get the number between 0 and 1\n","         # This are the transformations: for avoiding overfitting\n","         shear_range=0.2,\n","         zoom_range=0.2,\n","         horizontal_flip=True)\n","\n","# Connecting the above object with directory of training images, passing some parameters and storing in the variable\n","training_set= train_datagen.flow_from_directory( #.flow_from_dir, is a method of the class which would make the connections\n","         \"training_set\", #path for the training_images folder\n","         target_size=(64,64), #This is the finalSize when fed to model, greater than 64x64 makes the training very very long inefficient\n","         batch_size=32, #At a time batches of images is fed to the model, classic value is 32\n","         class_mode='binary') #Either binary or categorial - In this we have to specify the outcome, here 2-outcome(cat,dog) binary\n"]},{"cell_type":"markdown","id":"52d86f62-c55b-429c-8666-496a7dd4fa4e","metadata":{"id":"52d86f62-c55b-429c-8666-496a7dd4fa4e"},"source":["## Preprocessing the Test Set"]},{"cell_type":"code","execution_count":null,"id":"59542d8b-cf74-40e7-96bd-d1956f929a7f","metadata":{"id":"59542d8b-cf74-40e7-96bd-d1956f929a7f","outputId":"848e8b3e-0812-4d4b-d469-28b7ac20ada0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n"]}],"source":["# Now we only implimnet the rescaling in the testing images as bcz the model will required same scaled images as of trainingSet, just like in feature scaling we implinmet fit,transform in training and transform only in testing set\n","# Will not implimnet the rest transformation here, as they were expected to done only for better feature extraction and avoiding the overFitting\n","test_datagen=ImageDataGenerator(rescale=1./255)\n","\n","#Connecting the above object with the testing-dataset folder and storing in the variable\n","testing_set=test_datagen.flow_from_directory(\n","        \"test_set\",\n","         target_size=(64,64), #same size as of trainingSet\n","         batch_size=32, #same class as of training\n","         class_mode='binary') #same as we required 2 class"]},{"cell_type":"markdown","id":"fc444b7f-8be6-460b-a755-7d1e33efdcbb","metadata":{"id":"fc444b7f-8be6-460b-a755-7d1e33efdcbb"},"source":["# (3) Building the CNN model"]},{"cell_type":"markdown","id":"d6b25e56-05e2-4fbf-9620-e3f62166072a","metadata":{"id":"d6b25e56-05e2-4fbf-9620-e3f62166072a"},"source":["## Intilising the CNN variable"]},{"cell_type":"code","execution_count":null,"id":"f49d9443-c2a6-42bc-a4f0-a2ba11265c58","metadata":{"id":"f49d9443-c2a6-42bc-a4f0-a2ba11265c58"},"outputs":[],"source":["#Initialising the cnn variable which also a sequntialClass based variable/object just like ann with the help SequentialClass from models module\n","cnn=tf.keras.models.Sequential() #cnn object of SequentialClass\n"]},{"cell_type":"markdown","id":"c4bf27f9-167a-4d6f-8779-34b4000a8aec","metadata":{"id":"c4bf27f9-167a-4d6f-8779-34b4000a8aec"},"source":["## (i) Adding the Convolution layer"]},{"cell_type":"code","execution_count":null,"id":"4279c26e-bfd8-4c55-a4b0-9871fdd265ee","metadata":{"id":"4279c26e-bfd8-4c55-a4b0-9871fdd265ee"},"outputs":[],"source":["# Adding the convolution layer to the cnn variable with help of conv2d class from layers module with same add method adding the object of conv2d class\n","cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))\n","\n","# filters represent number of filers/featureExtracter/kernel for convolution\n","# Kernel_size means the size of those filters\n","# Till the output layer, use relu as activation function\n","# input_shape means input images shape, height,width,colorChannels fed to the model, this is written only at first layer for the connection of input layer with it\n"]},{"cell_type":"markdown","id":"aaa1264e-9238-4371-936d-312e219a5f52","metadata":{"id":"aaa1264e-9238-4371-936d-312e219a5f52"},"source":["## (ii) Pooling : Adding MaxPooling layer"]},{"cell_type":"code","execution_count":null,"id":"d3a8b388-bbef-45fd-b3c6-9340fc829754","metadata":{"id":"d3a8b388-bbef-45fd-b3c6-9340fc829754"},"outputs":[],"source":["# Adding the object of Maxpool2d class for adding the poolin layer\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","\n","# pool_size=2, means 2x2 matrix would be coverd while pooling the max\n","# stride means it would slide 2 pixels at once after every polling"]},{"cell_type":"markdown","id":"da056439-5e00-405d-afa9-cba1c23fc9f7","metadata":{"id":"da056439-5e00-405d-afa9-cba1c23fc9f7"},"source":["## Adding the Second Layer with convolution and maxpooling"]},{"cell_type":"code","execution_count":null,"id":"59c85a49-d01d-4412-9f3c-aee6576935e9","metadata":{"id":"59c85a49-d01d-4412-9f3c-aee6576935e9"},"outputs":[],"source":["# Adding the 2nd convolution layer to the cnn variable with help of conv2d class from layers module with same add method adding the object of conv2d class\n","cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n","#input_shape=[64,64,3] is deleted as bcz it written only in the first layer for connection with the input layer\n","\n","#Adding Maxpooling layer\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","\n","# Added the second convolution layer with maxpooling applied"]},{"cell_type":"markdown","id":"e29ed250-0aaf-4481-9773-9dbcdab8e2d3","metadata":{"id":"e29ed250-0aaf-4481-9773-9dbcdab8e2d3"},"source":["## (iii) Adding the Flatening layer"]},{"cell_type":"code","execution_count":null,"id":"aa2cd42d-0afc-4d63-a77b-913b2a141e72","metadata":{"id":"aa2cd42d-0afc-4d63-a77b-913b2a141e72"},"outputs":[],"source":["# Now we would pass the faltenning layer for flatenning the maps from above layers\n","cnn.add(tf.keras.layers.Flatten()) #Now this would fed as input to a nueral network"]},{"cell_type":"markdown","id":"f18b52c4-96fe-4469-91e8-4879dd5c3b68","metadata":{"id":"f18b52c4-96fe-4469-91e8-4879dd5c3b68"},"source":["## (iv) Now adding the Full Connection layer"]},{"cell_type":"code","execution_count":null,"id":"e68ee1fc-b28d-44b5-86aa-a9f9186b1b4a","metadata":{"id":"e68ee1fc-b28d-44b5-86aa-a9f9186b1b4a"},"outputs":[],"source":["# Now the falttend layer gives the many single numbers to every input single neuron as in input layer to a fullyConnected layer\n","# Now adding the fullyConnect layer help of DenseClass, adding the instance of Dense class with cnn variable\n","cnn.add(tf.keras.layers.Dense(units=128,activation=\"relu\")) #larger units is good in field of CV\n"]},{"cell_type":"markdown","id":"ce28a5cd-c9f6-4b4d-b81d-1ab0b0b13745","metadata":{"id":"ce28a5cd-c9f6-4b4d-b81d-1ab0b0b13745"},"source":["## (v) Output Layer"]},{"cell_type":"code","execution_count":null,"id":"45de6fe3-c87e-46a8-9c76-58905b70185e","metadata":{"id":"45de6fe3-c87e-46a8-9c76-58905b70185e"},"outputs":[],"source":["# Now from denseClass we add the output layer with activation function as softmax\n","# We need one neuron, for binary classifiction either 1 or 0\n","# For binary we need sigmoid as activation but for multiclass we need softmax\n","cnn.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))"]},{"cell_type":"markdown","id":"98d5fa16-3fd1-43af-b21a-728b220fea08","metadata":{"id":"98d5fa16-3fd1-43af-b21a-728b220fea08"},"source":["# (3) Training the CNN\n","### So till now we prepared the eyes and brain of the nural network help of CNN layers\n"]},{"cell_type":"markdown","id":"adb8a1ef-f63e-4306-9d3e-62805a53cf62","metadata":{"id":"adb8a1ef-f63e-4306-9d3e-62805a53cf62"},"source":["## (i) Compiling the CNN model\n"]},{"cell_type":"code","execution_count":null,"id":"839ac781-e35d-44d0-82cf-eadc81a1d262","metadata":{"id":"839ac781-e35d-44d0-82cf-eadc81a1d262"},"outputs":[],"source":["#Same way we did with ANN, as here the fullconnection part is also a neural network\n","#Same adam optimizer for stochastic gradient descent and binary cross entropy for binary classification\n","\n","cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"]},{"cell_type":"markdown","id":"901bd707-7cbc-43cf-a884-a8690a279711","metadata":{"tags":[],"id":"901bd707-7cbc-43cf-a884-a8690a279711"},"source":["## (ii) Training the model on the training set and evaluating it on the testSet"]},{"cell_type":"code","execution_count":null,"id":"50d59671-c5d9-4720-b2fc-5c25cd8a75a9","metadata":{"id":"50d59671-c5d9-4720-b2fc-5c25cd8a75a9","outputId":"29217e04-7c1a-4405-9a3a-8e3662f7324d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","250/250 [==============================] - 51s 196ms/step - loss: 0.6774 - accuracy: 0.5574 - val_loss: 0.6252 - val_accuracy: 0.6630\n","Epoch 2/25\n","250/250 [==============================] - 43s 174ms/step - loss: 0.6124 - accuracy: 0.6600 - val_loss: 0.5700 - val_accuracy: 0.7060\n","Epoch 3/25\n","250/250 [==============================] - 46s 183ms/step - loss: 0.5652 - accuracy: 0.7056 - val_loss: 0.5678 - val_accuracy: 0.7150\n","Epoch 4/25\n","250/250 [==============================] - 44s 174ms/step - loss: 0.5427 - accuracy: 0.7237 - val_loss: 0.5209 - val_accuracy: 0.7425\n","Epoch 5/25\n","250/250 [==============================] - 42s 170ms/step - loss: 0.5106 - accuracy: 0.7481 - val_loss: 0.4971 - val_accuracy: 0.7665\n","Epoch 6/25\n","250/250 [==============================] - 43s 170ms/step - loss: 0.4842 - accuracy: 0.7691 - val_loss: 0.4944 - val_accuracy: 0.7640\n","Epoch 7/25\n","250/250 [==============================] - 52s 208ms/step - loss: 0.4776 - accuracy: 0.7700 - val_loss: 0.4702 - val_accuracy: 0.7905\n","Epoch 8/25\n","250/250 [==============================] - 47s 186ms/step - loss: 0.4506 - accuracy: 0.7879 - val_loss: 0.4879 - val_accuracy: 0.7650\n","Epoch 9/25\n","250/250 [==============================] - 48s 193ms/step - loss: 0.4365 - accuracy: 0.7940 - val_loss: 0.4524 - val_accuracy: 0.7980\n","Epoch 10/25\n","250/250 [==============================] - 48s 191ms/step - loss: 0.4249 - accuracy: 0.8001 - val_loss: 0.4761 - val_accuracy: 0.7860\n","Epoch 11/25\n","250/250 [==============================] - 42s 169ms/step - loss: 0.4170 - accuracy: 0.8070 - val_loss: 0.4742 - val_accuracy: 0.7795\n","Epoch 12/25\n","250/250 [==============================] - 43s 170ms/step - loss: 0.4043 - accuracy: 0.8119 - val_loss: 0.4965 - val_accuracy: 0.7880\n","Epoch 13/25\n","250/250 [==============================] - 42s 169ms/step - loss: 0.3916 - accuracy: 0.8207 - val_loss: 0.4503 - val_accuracy: 0.7980\n","Epoch 14/25\n","250/250 [==============================] - 43s 171ms/step - loss: 0.3891 - accuracy: 0.8259 - val_loss: 0.4444 - val_accuracy: 0.7995\n","Epoch 15/25\n","250/250 [==============================] - 41s 165ms/step - loss: 0.3718 - accuracy: 0.8332 - val_loss: 0.4630 - val_accuracy: 0.7905\n","Epoch 16/25\n","250/250 [==============================] - 40s 161ms/step - loss: 0.3594 - accuracy: 0.8396 - val_loss: 0.4760 - val_accuracy: 0.7950\n","Epoch 17/25\n","250/250 [==============================] - 41s 163ms/step - loss: 0.3493 - accuracy: 0.8430 - val_loss: 0.4520 - val_accuracy: 0.8060\n","Epoch 18/25\n","250/250 [==============================] - 41s 164ms/step - loss: 0.3316 - accuracy: 0.8504 - val_loss: 0.4608 - val_accuracy: 0.8015\n","Epoch 19/25\n","250/250 [==============================] - 41s 164ms/step - loss: 0.3282 - accuracy: 0.8543 - val_loss: 0.4992 - val_accuracy: 0.7840\n","Epoch 20/25\n","250/250 [==============================] - 41s 163ms/step - loss: 0.3163 - accuracy: 0.8630 - val_loss: 0.4811 - val_accuracy: 0.7925\n","Epoch 21/25\n","250/250 [==============================] - 40s 161ms/step - loss: 0.3016 - accuracy: 0.8681 - val_loss: 0.5093 - val_accuracy: 0.7715\n","Epoch 22/25\n","250/250 [==============================] - 40s 160ms/step - loss: 0.2913 - accuracy: 0.8726 - val_loss: 0.4823 - val_accuracy: 0.8145\n","Epoch 23/25\n","250/250 [==============================] - 6745s 27s/step - loss: 0.2852 - accuracy: 0.8813 - val_loss: 0.4917 - val_accuracy: 0.8040\n","Epoch 24/25\n","250/250 [==============================] - 39s 156ms/step - loss: 0.2766 - accuracy: 0.8802 - val_loss: 0.4783 - val_accuracy: 0.8040\n","Epoch 25/25\n","250/250 [==============================] - 41s 163ms/step - loss: 0.2685 - accuracy: 0.8844 - val_loss: 0.4976 - val_accuracy: 0.7955\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x19346ddb880>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["cnn.fit(training_set, validation_data=testing_set,epochs=25)\n","# passed the trainin_set data , and passed the testing_set in validation_data for validating the testing_images togaether while training at each epochs\n"]},{"cell_type":"markdown","id":"4e60aee4-c32e-4fd0-a9ed-d9acb73a16fb","metadata":{"id":"4e60aee4-c32e-4fd0-a9ed-d9acb73a16fb"},"source":["# (4) Making Prediction,  \n","## Loading the image, and deploying in the model,so that it could predict whose image is of Cat or Dog"]},{"cell_type":"code","execution_count":null,"id":"7a623aaa-f7b8-4d2a-bab8-d184a3751894","metadata":{"id":"7a623aaa-f7b8-4d2a-bab8-d184a3751894","outputId":"ad6537a6-cc16-41fc-83b5-835e14fdd276"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 40ms/step\n","{'cats': 0, 'dogs': 1}\n","dog\n","1.0\n"]}],"source":["from keras.preprocessing import image  #import image module for loading the testing_image\n","# For this, we are creating the new variable which would hold the testing image from image module's function -> load_img\n","test_image = image.load_img('Image_2.jpg',target_size=(64,64)) #first given the path of image and set its size as of training images\n","\n","# Predict method demand the input as 2D array, so we have to convert image into array first, numpyArray\n","# For this a function is used from image module that is img_to_array function\n","test_image = image.img_to_array(test_image) #As its colored image, after conversion into array it become default 3d array\n","\n","# Our CNN model is not trained for single image, trained for treating the batches of image simultanoulsy that 32 at a time\n","# So the single image should be in a batch irrespetive how many, that's how the model recognizes the image\n","test_image = np.expand_dims(test_image,axis=0) #So from numpy, we could add a dimnesion, and for batch its at axis 0, as it come first then images\n","# Before above code, the dimension of image was (64x64x3) but after above code adding dimnesion to 0th axis or index , shape become (1x64x64x3)\n","\n","result = cnn.predict(test_image)\n","\n","print(training_set.class_indices)#represents the dictionary that model had encoded which to which class, so that we could use to recognixe the class\n","\n","# result[0][0]: hence test_image is in batch formate means first dimension is batch and rest comes in other dimnesion\n","# and test_image is equalised with result, so result become also in batchFormate, rest comes to other dimension\n","# so at 0th axis/index, only one dimnesion for batch, so first batch is excessed, that is result[0] {python: first is 0} and rest comes to next axis/index at index [0] becuase only one is there\n","if result[0][0]==1:\n","    prediction = 'dog'\n","else:\n","    prediction = 'cat'\n","\n","print(prediction)\n","print(result[0][0])"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}